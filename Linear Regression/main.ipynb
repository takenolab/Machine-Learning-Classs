{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55a1bf2b",
   "metadata": {},
   "source": [
    "- Learn to implement the model $f_{w,b}$ for linear regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b6a3de",
   "metadata": {},
   "source": [
    "#### Notation\n",
    "Here is a summary of some of the notation you will encounter.  \n",
    "<br />\n",
    "𝑎 - scalar, non bold\t<br/>\n",
    "𝐚 - vector, bold\tRegression\t<br/>\t\n",
    "𝐱 \t- Training Example feature values x_train <br/>\n",
    "𝐲\t- Training Example targets \ty_train <br/>\n",
    "𝑥(𝑖),𝑦(𝑖)𝑖𝑡ℎ -  Training Example\tx_i, y_i <br/>\n",
    "m - Number of training examples\tm <br/>\n",
    "𝑤 - parameter: weight\tw <br/>\n",
    "𝑏 - parameter: bias\tb <br/>\n",
    "𝑓𝑤,𝑏(𝑥(𝑖)) - The result of the model evaluation at  𝑥(𝑖) parameterized  by  𝑤,𝑏 :  𝑓𝑤,𝑏(𝑥(𝑖))=𝑤𝑥(𝑖)+𝑏\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdbaaf5",
   "metadata": {},
   "source": [
    "#### Packages\n",
    "- Numpy for scientific computing\n",
    "- Matplotlib,for plotting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac38fb7c",
   "metadata": {},
   "source": [
    "##### problem\n",
    "- A grocery store wants to predict the price\n",
    "of rice bags based on their weight. You are given \n",
    "the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4911c1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b49110",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "#### 4 - Refresher on linear regression\n",
    "\n",
    "In this we will fit the linear regression parameters $(w,b)$ to our dataset.\n",
    "- The model function for linear regression, which is a function that maps from `x`  to `y`  is represented as $$f_{w,b}(x) = wx + b$$\n",
    "    \n",
    "\n",
    "- To train a linear regression model, we  want to find the best $(w,b)$ parameters that fit our dataset.  \n",
    "\n",
    "    - To compare how one choice of $(w,b)$ is better or worse than another choice, we can evaluate it with a cost function $J(w,b)$\n",
    "      - $J$ is a function of $(w,b)$. That is, the value of the cost $J(w,b)$ depends on the value of $(w,b)$.\n",
    "  \n",
    "    - The choice of $(w,b)$ that fits our data the best is the one that has the smallest cost $J(w,b)$.\n",
    "\n",
    "\n",
    "- To find the values $(w,b)$ that gets the smallest possible cost $J(w,b)$, we can use a method called **gradient descent**. \n",
    "  - With each step of gradient descent, our parameters $(w,b)$ come closer to the optimal values that will achieve the lowest cost $J(w,b)$.\n",
    "  \n",
    "\n",
    "- The trained linear regression model can then take the input feature $x$ and output a prediction $f_{w,b}(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb3dd4d",
   "metadata": {},
   "source": [
    "<a name=\"5\"></a>\n",
    "## 5 - Compute Cost\n",
    "\n",
    "Gradient descent involves repeated steps to adjust the value of our parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$.\n",
    "- At each step of gradient descent, it will be helpful for us to monitor our progress by computing the cost $J(w,b)$ as $(w,b)$ gets updated. \n",
    "- In this section, we will implement a function to calculate $J(w,b)$ so that we can check the progress of our gradient descent implementation.\n",
    "\n",
    "#### Cost function\n",
    "for one variable, the cost function for linear regression $J(w,b)$ is defined as\n",
    "\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2$$ \n",
    "\n",
    "- we can think of $f_{w,b}(x^{(i)})$ as the model's prediction\n",
    "- $m$ is the number of training examples in the dataset\n",
    "\n",
    "#### Model prediction\n",
    "\n",
    "- For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is representented as:\n",
    "\n",
    "$$ f_{w,b}(x^{(i)}) = wx^{(i)} + b$$\n",
    "\n",
    "This is the equation for a line, with an intercept $b$ and a slope $w$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0981fd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR:\n",
    "    def __init__(self, lr=0.01, iters=1000):\n",
    "        self.lr = lr\n",
    "        self.epochs = iters\n",
    "        self.weight = 0.0\n",
    "        self.bias = 0.0\n",
    "        self.mse_history = []\n",
    "\n",
    "    def train(self, X, y):\n",
    "        np.random.seed(42)\n",
    "        self.weight = np.random.randn()\n",
    "        self.bias = np.random.randn()\n",
    "        m = X.shape[0]\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            y_pred = self.weight * X + self.bias\n",
    "            error = y_pred - y\n",
    "\n",
    "            dj_dw = (1/m) * np.dot(error, X)\n",
    "            dj_db = (1/m) * np.sum(error)\n",
    "\n",
    "            self.weight -= self.lr * dj_dw\n",
    "            self.bias -= self.lr * dj_db\n",
    "\n",
    "            cost = (1/(2*m)) * np.sum(error**2)\n",
    "            self.mse_history.append(cost)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.weight * X + self.bias\n",
    "\n",
    "    def get_cost_history(self):\n",
    "        return self.mse_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
