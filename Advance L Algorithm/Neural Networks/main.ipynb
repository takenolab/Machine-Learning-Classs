{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998a1034",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3846a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109304a",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58de74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for no rain 1 for rain\n",
    "X = np.array([[60, 65, 70, 75, 80, 85, 90, 95, 100]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03751a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 9) (1, 9)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80950927",
   "metadata": {},
   "source": [
    "# Neural Networks: Neurons, Layers, and Forward Propagation\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Layer\n",
    "\n",
    "Artificial neural networks are made of **layers** of *neurons* (also called nodes). A **neuron** is a simple computing unit that takes one or more inputs, multiplies each by a weight, adds a bias, and then applies a non-linear *activation function*.\n",
    "\n",
    "Mathematically, for one neuron with input vector $x$ and weights $w$, we compute:\n",
    "\n",
    "$$\n",
    "z = w^\\mathsf{T} x + b, \n",
    "\\qquad \n",
    "a = g(z),\n",
    "$$\n",
    "\n",
    "where $g(\\cdot)$ is an activation function (e.g., sigmoid or ReLU). Each neuron’s output $a$ is a real number passed on to the next layer.\n",
    "\n",
    "**Example:** A sigmoid neuron computes\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) \\;=\\; \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "A **layer** is a group of such neurons operating in parallel. Layers are stacked in order:\n",
    "\n",
    "* **Input layer**: holds the raw features (no computation).\n",
    "* **Hidden layers**: perform successive transformations.\n",
    "* **Output layer**: produces the final prediction.\n",
    "---\n",
    "\n",
    "## More Complex Neural Networks\n",
    "\n",
    "By stacking more hidden layers, a network becomes *deeper* and more powerful. Each additional layer allows the network to learn more complex, hierarchical features from data. For example, in image recognition:\n",
    "\n",
    "* The first layer might detect edges.\n",
    "* The next layer might detect simple shapes.\n",
    "* Later layers might detect complete objects.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Deep Networks**: A network with two or more hidden layers is called a *deep neural network*.\n",
    "* **Layer Types**: Besides fully-connected (dense) layers, modern networks can include convolutional layers, pooling layers, etc.\n",
    "* **Trade-offs**: Deeper networks solve harder problems but require more data and computation to train.\n",
    "\n",
    "---\n",
    "\n",
    "## Inference: Forward Propagation\n",
    "\n",
    "**Forward propagation** (inference) is how the network computes its output (prediction) given an input. We feed the input vector $X$ into the first layer and compute outputs layer by layer until the final layer.\n",
    "\n",
    "For each layer $\\ell$, we perform:\n",
    "\n",
    "1. **Linear Step**\n",
    "\n",
    "   $$\n",
    "   Z^{[\\ell]} = W^{[\\ell]}\\,A^{[\\ell-1]} + b^{[\\ell]},\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   * $A^{[0]} = X$ (the input data).\n",
    "   * $W^{[\\ell]}$ and $b^{[\\ell]}$ are the weights and bias of layer $\\ell$.\n",
    "2. **Activation**\n",
    "\n",
    "   $$\n",
    "   A^{[\\ell]} = g\\bigl(Z^{[\\ell]}\\bigr),\n",
    "   $$\n",
    "\n",
    "   where $g(\\cdot)$ is the chosen activation function (e.g., sigmoid, ReLU).\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Two-Layer Network\n",
    "\n",
    "1. **Hidden layer** ($\\ell = 1$):\n",
    "\n",
    "   $$\n",
    "   Z^{[1]} = W^{[1]} X + b^{[1]},\n",
    "   \\qquad\n",
    "   A^{[1]} = g\\bigl(Z^{[1]}\\bigr).\n",
    "   $$\n",
    "2. **Output layer** ($\\ell = 2$):\n",
    "\n",
    "   $$\n",
    "   Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]},\n",
    "   \\qquad\n",
    "   A^{[2]} = g\\bigl(Z^{[2]}\\bigr).\n",
    "   $$\n",
    "3. The final output $A^{[2]}$ is the network’s prediction.\n",
    "\n",
    "In summary, at each layer:\n",
    "\n",
    "$$\n",
    "Z = W\\,(\\text{previous activations}) + b, \n",
    "\\quad\n",
    "A = g(Z).\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6c901",
   "metadata": {},
   "source": [
    "## Neurons and Layers (Lab)\n",
    "\n",
    "Below is a simple NumPy implementation of a single layer’s forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5ee53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def linear_forward(X, W, b):\n",
    "    \n",
    "    Z = np.dot(W, X) + b \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(X, W, b, activation=\"sigmoid\"):\n",
    "    \n",
    "    Z = linear_forward(X, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5592c",
   "metadata": {},
   "source": [
    "#### Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fcc41",
   "metadata": {},
   "source": [
    "- Now that we’ve done inference in NumPy, let’s see how to load the same data into TensorFlow so we can build and train neural networks more easily.\n",
    "\n",
    "1. Converting NumPy arrays to TensorFlow Tensors\n",
    "- TensorFlow works best with tf.Tensor or tf.data.Dataset objects. We can convert our NumPy arrays directly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007474d",
   "metadata": {},
   "source": [
    "```python\n",
    "X_np = np.array([[60,  65,  70,  75,  80,  85,  90,  95, 100]]).T  # shape: (9, 1)\n",
    "y_np = np.array([[0,   0,   0,   0,   1,   1,   1,   1,   1]]).T    # shape: (9, 1)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X_tf = tf.constant(X_np, dtype=tf.float32)  # shape: (9, 1)\n",
    "y_tf = tf.constant(y_np, dtype=tf.float32)  # shape: (9, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e765c57",
   "metadata": {},
   "source": [
    "#### Building a Neural Network with Tensorflow\n",
    "\n",
    "- we’ll define a simple neural network in TensorFlow to predict “rain/no rain” from temperature.\n",
    "- We’ll use tf.keras.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865d1b4",
   "metadata": {},
   "source": [
    "#### Define the Model Architecture\n",
    "We’ll build a tiny network with:\n",
    "\n",
    "- One input feature (temperature)\n",
    "\n",
    "- One hidden layer with 2 neurons (sigmoid activation)\n",
    "\n",
    "- One output neuron (sigmoid activation to produce a probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad78e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=2, activation='sigmoid',  input_shape=(1,), name='hidden_layer'),\n",
    "    Dense(units=1, activation='sigmoid', name='output_layer')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a5a467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
