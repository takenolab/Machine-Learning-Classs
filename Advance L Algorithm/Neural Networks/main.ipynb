{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "998a1034",
   "metadata": {},
   "source": [
    "#### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3846a77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9109304a",
   "metadata": {},
   "source": [
    "#### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58de74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 for no rain 1 for rain\n",
    "X = np.array([[60, 65, 70, 75, 80, 85, 90, 95, 100]])\n",
    "y = np.array([[0, 0, 0, 0, 1, 1, 1, 1, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03751a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80950927",
   "metadata": {},
   "source": [
    "# Neural Networks: Neurons, Layers, and Forward Propagation\n",
    "\n",
    "---\n",
    "\n",
    "## Neural Network Layer\n",
    "\n",
    "Artificial neural networks are made of **layers** of *neurons* (also called nodes). A **neuron** is a simple computing unit that takes one or more inputs, multiplies each by a weight, adds a bias, and then applies a non-linear *activation function*.\n",
    "\n",
    "Mathematically, for one neuron with input vector $x$ and weights $w$, we compute:\n",
    "\n",
    "$$\n",
    "z = w^\\mathsf{T} x + b, \n",
    "\\qquad \n",
    "a = g(z),\n",
    "$$\n",
    "\n",
    "where $g(\\cdot)$ is an activation function (e.g., sigmoid or ReLU). Each neuron’s output $a$ is a real number passed on to the next layer.\n",
    "\n",
    "**Example:** A sigmoid neuron computes\n",
    "\n",
    "$$\n",
    "a = \\sigma(z) \\;=\\; \\frac{1}{1 + e^{-z}}.\n",
    "$$\n",
    "\n",
    "A **layer** is a group of such neurons operating in parallel. Layers are stacked in order:\n",
    "\n",
    "* **Input layer**: holds the raw features (no computation).\n",
    "* **Hidden layers**: perform successive transformations.\n",
    "* **Output layer**: produces the final prediction.\n",
    "---\n",
    "\n",
    "## More Complex Neural Networks\n",
    "\n",
    "By stacking more hidden layers, a network becomes *deeper* and more powerful. Each additional layer allows the network to learn more complex, hierarchical features from data. For example, in image recognition:\n",
    "\n",
    "* The first layer might detect edges.\n",
    "* The next layer might detect simple shapes.\n",
    "* Later layers might detect complete objects.\n",
    "\n",
    "Key points:\n",
    "\n",
    "* **Deep Networks**: A network with two or more hidden layers is called a *deep neural network*.\n",
    "* **Layer Types**: Besides fully-connected (dense) layers, modern networks can include convolutional layers, pooling layers, etc.\n",
    "* **Trade-offs**: Deeper networks solve harder problems but require more data and computation to train.\n",
    "\n",
    "---\n",
    "\n",
    "## Inference: Forward Propagation\n",
    "\n",
    "**Forward propagation** (inference) is how the network computes its output (prediction) given an input. We feed the input vector $X$ into the first layer and compute outputs layer by layer until the final layer.\n",
    "\n",
    "For each layer $\\ell$, we perform:\n",
    "\n",
    "1. **Linear Step**\n",
    "\n",
    "   $$\n",
    "   Z^{[\\ell]} = W^{[\\ell]}\\,A^{[\\ell-1]} + b^{[\\ell]},\n",
    "   $$\n",
    "\n",
    "   where:\n",
    "\n",
    "   * $A^{[0]} = X$ (the input data).\n",
    "   * $W^{[\\ell]}$ and $b^{[\\ell]}$ are the weights and bias of layer $\\ell$.\n",
    "2. **Activation**\n",
    "\n",
    "   $$\n",
    "   A^{[\\ell]} = g\\bigl(Z^{[\\ell]}\\bigr),\n",
    "   $$\n",
    "\n",
    "   where $g(\\cdot)$ is the chosen activation function (e.g., sigmoid, ReLU).\n",
    "\n",
    "---\n",
    "\n",
    "### Example: Two-Layer Network\n",
    "\n",
    "1. **Hidden layer** ($\\ell = 1$):\n",
    "\n",
    "   $$\n",
    "   Z^{[1]} = W^{[1]} X + b^{[1]},\n",
    "   \\qquad\n",
    "   A^{[1]} = g\\bigl(Z^{[1]}\\bigr).\n",
    "   $$\n",
    "2. **Output layer** ($\\ell = 2$):\n",
    "\n",
    "   $$\n",
    "   Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]},\n",
    "   \\qquad\n",
    "   A^{[2]} = g\\bigl(Z^{[2]}\\bigr).\n",
    "   $$\n",
    "3. The final output $A^{[2]}$ is the network’s prediction.\n",
    "\n",
    "In summary, at each layer:\n",
    "\n",
    "$$\n",
    "Z = W\\,(\\text{previous activations}) + b, \n",
    "\\quad\n",
    "A = g(Z).\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba6c901",
   "metadata": {},
   "source": [
    "## Neurons and Layers (Lab)\n",
    "\n",
    "Below is a simple NumPy implementation of a single layer’s forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee53b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def linear_forward(X, W, b):\n",
    "    \n",
    "    Z = np.dot(W, X) + b \n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6cd685",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(X, W, b, activation=\"sigmoid\"):\n",
    "    \n",
    "    Z = linear_forward(X, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = 1 / (1 + np.exp(-Z))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported activation function\")\n",
    "    \n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e5592c",
   "metadata": {},
   "source": [
    "#### Data in TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522fcc41",
   "metadata": {},
   "source": [
    "- Now that we’ve done inference in NumPy, let’s see how to load the same data into TensorFlow so we can build and train neural networks more easily.\n",
    "\n",
    "1. Converting NumPy arrays to TensorFlow Tensors\n",
    "- TensorFlow works best with tf.Tensor or tf.data.Dataset objects. We can convert our NumPy arrays directly:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d007474d",
   "metadata": {},
   "source": [
    "```python\n",
    "X_np = np.array([[60,  65,  70,  75,  80,  85,  90,  95, 100]]).T  # shape: (9, 1)\n",
    "y_np = np.array([[0,   0,   0,   0,   1,   1,   1,   1,   1]]).T    # shape: (9, 1)\n",
    "\n",
    "# Convert to TensorFlow tensors\n",
    "X_tf = tf.constant(X_np, dtype=tf.float32)  # shape: (9, 1)\n",
    "y_tf = tf.constant(y_np, dtype=tf.float32)  # shape: (9, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e765c57",
   "metadata": {},
   "source": [
    "#### Building a Neural Network with Tensorflow\n",
    "\n",
    "- we’ll define a simple neural network in TensorFlow to predict “rain/no rain” from temperature.\n",
    "- We’ll use tf.keras.Sequential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865d1b4",
   "metadata": {},
   "source": [
    "#### Define the Model Architecture\n",
    "We’ll build a tiny network with:\n",
    "\n",
    "- One input feature (temperature)\n",
    "\n",
    "- One hidden layer with 2 neurons (sigmoid activation)\n",
    "\n",
    "- One output neuron (sigmoid activation to produce a probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7ad78e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Rhodrick\\Machine Learning\\TakenoLab\\getting-started-with-supervised-Machine-Learning-Regression-Algorithm\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ hidden_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │             \u001b[38;5;34m4\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ output_layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m3\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> (28.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m7\u001b[0m (28.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">7</span> (28.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m7\u001b[0m (28.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(units=2, activation='sigmoid',  input_shape=(1,), name='hidden_layer'),\n",
    "    Dense(units=1, activation='sigmoid', name='output_layer')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae75316",
   "metadata": {},
   "source": [
    "#### Manual implmentation of the above neural network with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19898af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SampleNeuralNetwork:\n",
    "    def __init__(self, seed:int=42):\n",
    "        np.random.seed(seed)\n",
    "        self.layer_dims ={}\n",
    "        #Layer dimensions:\n",
    "        # Input -> Hidden -> Output\n",
    "        self.layer_dims ={\n",
    "            'L0':1,  # one input feature (temperature)\n",
    "            'L1':2,   # hidden layer has 2 neurons\n",
    "            'L2':1\n",
    "        }\n",
    "        self.params ={}\n",
    "        self.params['W1'] = np.random.randn(\n",
    "            self.layer_dims['L1'], self.layer_dims['L0']\n",
    "        ) * 0.01\n",
    "        self. params['b1'] = np.zeros(\n",
    "        (self.layer_dims['L1'],1)\n",
    "        )\n",
    "        self.params[\"W2\"] = np.random.randn(\n",
    "            self.layer_dims[\"L2\"],\n",
    "            self.layer_dims[\"L1\"]\n",
    "        ) * 0.01\n",
    "        self.params[\"b2\"] = np.zeros(\n",
    "            (self.layer_dims[\"L2\"], 1)\n",
    "        )\n",
    "        @staticmethod\n",
    "        def sigmoid(self, z:np.ndarray) -> np.ndarray:\n",
    "            return 1/(1+np.exp(-z))\n",
    "        def _linear_forward(self, A_prev:np.ndarray, W:np.ndarray, b:np.ndarray) -> np.ndarray:\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            return Z\n",
    "        def _forward_activation(self, A_prev:np.ndarray, W:np.ndarray, b:np.ndarray) -> (np.ndarray, np.ndarray):\n",
    "            Z = self._linear_forward(A_prev,W,b)\n",
    "            A = self.sigmoid(Z)\n",
    "            return A,Z\n",
    "        def forward_propagation(self, x: float) -> float:\n",
    "            A0 = np.array([[x]]) \n",
    "            W1 = self.params[\"W1\"]\n",
    "            b1 = self.params[\"b1\"] \n",
    "            A1, Z1 = self._forward_activation(A0, W1, b1)\n",
    "\n",
    "            W2 = self.params[\"W2\"] \n",
    "            b2 = self.params[\"b2\"] \n",
    "            A2, Z2 = self._forward_activation(A1, W2, b2)\n",
    "            return float(A2)\n",
    "        def summary(self):\n",
    "            print(\"Network architecture:\")\n",
    "            print(f\"  Input layer size   (L0) = {self.layer_dims['L0']}\")\n",
    "            print(f\"  Hidden layer size  (L1) = {self.layer_dims['L1']}\")\n",
    "            print(f\"  Output layer size  (L2) = {self.layer_dims['L2']}\")\n",
    "            print(\"\\nParameter shapes:\")\n",
    "            print(f\"  W1: {self.params['W1'].shape}, b1: {self.params['b1'].shape}\")\n",
    "            print(f\"  W2: {self.params['W2'].shape}, b2: {self.params['b2'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934c081",
   "metadata": {},
   "source": [
    "### Alternative to the Sigmoid Activation Function\n",
    "\n",
    "#### 1. ReLU (Rectified Linear Unit)\n",
    "\n",
    "The **Rectified Linear Unit (ReLU)** is one of the most popular activation functions used in deep learning. It introduces non-linearity in the model while remaining simple and computationally efficient.\n",
    " \n",
    "- ReLU is a **piecewise linear function** that outputs the input directly if it is positive; otherwise, it returns zero.\n",
    "\n",
    "Mathematically: <br>\n",
    "\n",
    "- f(x) =` max(0, x)` </p> </tab>\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0cdd4e",
   "metadata": {},
   "source": [
    "#### Choosing activation Function\n",
    "- For output layers\n",
    "\n",
    "\n",
    "- For hidden layers\n",
    "\n",
    "\n",
    "\n",
    "##### Why activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec18fd4",
   "metadata": {},
   "source": [
    "### Multi classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19053997",
   "metadata": {},
   "source": [
    "#### Softmax Regression(Algorithm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9146e728",
   "metadata": {},
   "source": [
    "#### Neural Network with softmax output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726717a",
   "metadata": {},
   "source": [
    "#### implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acdbf1c",
   "metadata": {},
   "source": [
    "### Multi classification with multiple outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941dd0d7",
   "metadata": {},
   "source": [
    "#### multi-label classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e166e",
   "metadata": {},
   "source": [
    "### Optimization Algorithm\n",
    "- Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45165483",
   "metadata": {},
   "source": [
    "### Layer types\n",
    "- Dense layer\n",
    "\n",
    "\n",
    "- Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5b64f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
